import os
import numpy as np
import pandas as pd
from ReplayMemory import ReplayMemory
from keras.models import Model
from keras.layers import Conv2D, Flatten, Dense, Lambda, Input, multiply
from keras import backend as K
import time
from Environments import WrapperGym#, WrapperDoom
import tensorflow as tf
from tensorflow import set_random_seed
from utils import printd, folder_exists
import utils

#Setting all random seeds to a value known
seed=1
np.random.seed(seed)
set_random_seed(seed)
#Silencing tensorflow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
#The GPU id to use
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="0"

#Setting the DEBUG lvl of the function printd (utils.py)
utils.DEBUG = True
utils.DEBUG_lvl = 1

class AgentDQN:
    """
    Class that create the DQN agent that will interact with an environment.
    """

    def __init__(self,
                 env='PongNoFrameskip-v4',
                 num_simul_frames=10000000,
                 discount_rate=0.99,
                 lr=0.00025,
                 epsilon=1.0,
                 e_min=0.1,
                 e_lin_decay=1000000,
                 linear_decay_mode=True,
                 e_exp_decay=300,
                 target_update=10000,
                 num_states_stored=1000000,
                 batch_size=32,
                 input_size=(84,84),
                 history_size=4,
                 num_random_play=50000,
                 load=False,
                 weights_load_name="",
                 loss_type="huber",
                 optimizer="rmsprop",
                 path_save_plot="Plot",
                 path_save_weights="Weights",
                 silent_mode = False
                 ):
        """

        :param:
            env : str (default PongNoFrameskip-v4 (atari gym environment [see gym documentation for more details])
                The name of the environment where the agent will interact.
            num_simul_frames : int (Default 10 000 000)
                Total number of frames that the agent will be trained.
            discount_rate : float (Default: 0.99)
                Discount rate gamma (RL parameter).
            lr : float (Default: 0.00025)
                Neural Network learning rate.
            epsilon : float (Default: 1.0 (100% of exploration))
                Initial value of random actions (exploration) chosen by the agent using the policy e-greedy.
            e_min : float (Default: 0.1 (10% of exploration))
                Final value of random actions (exploration) chosen by the agent using the policy e-greedy.
            e_lin_decay : int (Default: 1 000 000)
                Number of frames for epsilon reach its final value linearly (e_min).
            linear_decay_mode : bool (Default: True - linear decay enable).
                Enables the linear decay mode of epsilon or disable(thus enabling the exponential decay mode).
            e_exp_decay : int (Default:300 [ie 63.2% of decay in 300 episodes])
                Exponential decay rate in EPISODES (Bigger values slowly is the decay since exp^[-1/e.decay]).
            target_update : int (Default:10 000)
                Number of frames that the parameters of Q_hat will be update with the parameters of Q_value
                [See the DQN paper for more details].
            num_states_stored : int (Default: 1 000 000)
                Number of states stored in the replay memory.
            batch_size : int (Default: 32)
                Number of samples uniformly sampled from the replay memory to do the Neural Network (NN) training.
            input_size : tuple (int) (Default: (84,84))
                Width x Height of the input frame (image) that will be send to the Neural Network.
            history_size : int (Default: 4)
                Number of sequential frames that will be stacked together to form the input volume to the NN.
            num_random_play : int (Default: 50 000)
                Number of states generate by actions chosen randomly that will be stored in the replay memory
                before the DQN algorithm trains begin.
            load : bool (Default: False)
                Varible that controls if it's to load the weights from a external .h5 file generated by another
                simualtion.
            weights_load_name : str (Default: "")
                Name of the .h5 file with the weights of the Network to be loaded.
            loss_type : str (Default: "hubber")
                Name of the type of loss function that will be used to train the DQN Network. There are two possible
                types: "huber" and "MSE".
            optimizer : str (Default: "rmsprop")
                Name of the type of optimizer that will be used to train the DQN Network. There are two possible
                types: "rmsprop" and "adam". The first one uses the setting described on the DQN paper, the second
                uses the tensorflow/keras default parameters.
            path_save_plot : str (Default: "Plot")
                Name of the folder where will be saved the .csv file with the algorithm information.
            path_save_plot : str (Default: "Weights")
                Name of the folder where will be saved the .h5 file with the Neural Network Weights.
            silent_mode : bool (Default : False)
                If it's active no message will be displayed on the prompt.

        """
        #Setting the silent mode
        if silent_mode:
            utils.DEBUG = False
        if "doom" in env.lower():
            pass
            #self.env = WrapperDoom.WrapperDoom(env)
        else:
            self.env = WrapperGym.WrapperGym(env)

        #Total number of frames that the simulation will run
        self.num_simul_frames = num_simul_frames
        #Counter to the total number of steps
        self.steps_cont = 0
        #Total number of actions possible inside the environment
        self.actions_num = self.env.numberOfActions()
        #Size(Width x Height) of the input images
        self.input_size = input_size
        #Defining the input of the network (Input volume shape)
        self.input_shape = self.input_size+(history_size,)

        #=========Learning Parameters===========#
        self.discount_rate = discount_rate
        self.lr = lr
        self.epsilon = epsilon
        self.e_min = e_min
        self.e_exp_decay = e_exp_decay
        self.e_lin_decay = e_lin_decay
        self.linear_decay_mode = linear_decay_mode
        self.target_update = target_update
        self.Q_value = self.initalize_network("Q_value",load,weights_load_name)
        self.Q_hat = self.initalize_network("Q_hat")
        self.update_Q_hat()
        self.loss_type = loss_type
        self.optimizer = optimizer

        #Clipping the error between the interval of 0.0 and 1.0 to compute the Huber Loss
        self.error_clip = 1.0

        #Inicializing the Replay memory
        self.batch_size = batch_size
        self.replay_memory = ReplayMemory(num_states_stored,batch_size)
        self.history_size = history_size
        #Number of random plays to fill the replay memory before the RL-algorithm begins
        self.num_random_play = num_random_play

        #======Auxiliary variables to plot or control=======#
        self.start_episode = 0
        self.i_episode = 0
        self.loss_value = 0.0
        self.q_rate = 0.0
        self.values_dict = {"Rewards":[],"Loss":[],"Q_value":[],"Num_frames":[], "Time":[]}
        self.image_array=[]
        self.reward_100 = 0

        self.path_save_plot = os.path.join(os.path.dirname(os.path.realpath(__file__)), path_save_plot)
        self.path_save_weights = os.path.join(os.path.dirname(os.path.realpath(__file__)), path_save_weights)

        self.initialize_graph()
        printd("==================================================================================================")
        printd("Initializing the DQN algorithm with the following parameters:")
        printd("    Number of total frames that will be simulated: {} frame(s)".format(self.num_simul_frames))
        printd("    Discount rate: {}".format(self.discount_rate))
        printd("    Initial Episilon: {}".format(self.epsilon))
        printd("    Final Episilon: {}".format(self.e_min))
        if self.linear_decay_mode:
            printd("    Linear Decay mode is activated!")
            printd("    The final Episilon will be reached in: {} frame(s)".format(self.e_lin_decay))
        else:
            printd("    Exponential Decay mode is activated!")
            printd("    The final Episilon will be reached in approximately: {} episode(s)".format(self.e_exp_decay*5))
        printd("    Learning rate: {}".format(self.lr))
        printd("    Batch size: {}".format(self.batch_size))
        printd("    State shape: {}".format(self.input_shape))
        printd("    The Network will have the {} loss".format(self.loss_type.upper()))
        printd("    The Network will be trained using {} optimizer".format(self.optimizer.upper()))
        printd("    The Target Network will be updated every: {} frame(s)".format(self.target_update))
        printd("    The Replay Memory will store: {} state(s)".format(num_states_stored))
        printd("    Number approximately of random plays before training: {} state(s)".format(self.num_random_play))
        printd("    The information will be saved in:{}".format(self.path_save_plot))
        printd("    The NN weights will be saved in:{}".format(self.path_save_weights))
        printd("==================================================================================================")





    def initalize_network(self, name,load=False,weights_load_name=""):
        """
        Function that creates the neural network and the Tensorflow session.

        This function creates the Neural Network (NN) with the parameters given by the DQN paper (Nature) using
        Keras framework together with Tensorflow. Before the Input volume enters in the network, its pixels are
        normalized (see lambda function on the code for more details). The network receives 2 inputs, the first
        is the image volume, and the last is a "Mask/Filter". The Mask has the function of filter the desired Q
        values and clear the rest.

        :param
            name : str
                The NN's name
            load : bool (optional)
                Parameter that decides if a NN will be loaded from a external file or not (default is False).

        :return
            nothing
        """
        frames_input = Input(self.input_shape, name=name)
        actions_input = Input((self.actions_num,), name='filter')
        lamb = Lambda(lambda x: (2 * x - 255) / 255.0, )(frames_input)
        conv_1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(lamb)
        conv_2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(conv_1)
        conv_3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(conv_2)
        conv_flattened = Flatten()(conv_3)
        hidden = Dense(512, activation='relu')(conv_flattened)
        output = Dense(self.actions_num)(hidden)
        filtered_output = multiply([output,actions_input])
        model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)
        self.sess = tf.Session()
        K.set_session(self.sess)
        if(load):
            if weights_load_name != "":
                printd("Loading the Model {}!".format(weights_load_name),lvl=1)
                model.load_weights(os.path.join(self.path_save_weights,))
            else:
                printd("Load option is True but no path to a valid .h5 file was given"
                       " therefore no weights was loadaded!")
        if (utils.DEBUG and utils.DEBUG_lvl >=2):
            model.summary()
        return model


    def initialize_graph(self):
        """
        Function that initializes the tensorflow graph that computes the NN training.

        This function creates the tensorflow graph that will compute the NN error and training. The network
        can be trained by two different losses, hubber loss (default) and mse (Mean Squared Error), the type of
        loss used is defined on the initialization of this class by the parameter "loss_type". This function allows
        you to choose between two optimizers: RMSProp (with the settings given by the DQN Paper) and ADAM (with
        tensorflow defaults arguments), this choice is also made on the initialization by the parameter "optimizer".
        OBS: The tensorflow graph is fed by dictionary in the function train_dqn().

        :param
            nothing

        :return
            nothing

        """
        #Defining the tensors variables (placeholds)
        self.state = tf.placeholder(tf.uint8, [None] + list(self.input_shape))
        self.action = tf.placeholder(tf.int32, [None])
        self.reward = tf.placeholder(tf.float32, [None])
        self.state_next= tf.placeholder(tf.uint8, [None] + list(self.input_shape))
        self.done = tf.placeholder(tf.float32, [None])
        #Defining the operations
        state_float = tf.cast(self.state, tf.float32)
        state_next_float = tf.cast(self.state_next, tf.float32)
        act_one_hot = tf.one_hot(self.action, self.actions_num, on_value=1.0, off_value=0.0)
        self.mask_one = tf.ones_like(act_one_hot, tf.float32)
        current_q = tf.reduce_sum(self.Q_value([state_float, act_one_hot]),axis=1)
        prediction = self.Q_hat([state_next_float, self.mask_one])
        target_q = tf.reduce_max(prediction, axis=1)
        #Computing the NN ERROR as descrived in the DQN paper.
        target_val = tf.stop_gradient(self.reward + (self.discount_rate * target_q) * (1 - self.done))
        if "huber" in self.loss_type:
            # Computing the Huber Loss
            self.loss_train = tf.losses.huber_loss(current_q,target_val)
        elif "mse" in self.loss_type:
            #Computing the MSE loss
            self.loss_train = tf.losses.mean_squared_error(target_val,current_q)
        if "rms" in self.optimizer.lower():
            #Using RMSprop with DQN paper's parameters
            self.train_op = tf.train.RMSPropOptimizer(
                 self.lr, decay=0.95, momentum=0.0, epsilon=0.01).minimize(self.loss_train)
        elif "adam" in self.optimizer.lower():
            #Using the Adam  optimizer with default parameters
            self.train_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss_train)
        #Initializing the variables
        self.sess.run(tf.global_variables_initializer())


    def update_Q_hat(self):
        """
        Function that updates the values of Q_hat with the values of Q_value at each N (:param target_update) steps.
        :param
            nothing
        :return
            nothing
        """
        self.Q_hat.set_weights(self.Q_value.get_weights())


    def e_greddy_action(self, state):
        """
        Function that selects an action with base on the e-greedy police.

        :param:
            state : input volume of shape input_shape
                A volume compund of a set of states (images) of depth ":param history_size".
        :return
            nothing
        """

        # Gets a random action if the variable self.epsilon is less than a random variable (distributed between 0
        # and 1)
        if np.random.random() < self.epsilon:
            action = np.random.choice(np.arange(self.actions_num))
        # Otherwise the algorithm computes the Q value of each action possible for that state and pick the bigger.
        else:
            prediction = self.Q_value.predict_on_batch([state,np.ones((1,self.actions_num))])
            self.q_rate += np.amax(prediction)
            action = np.argmax(prediction)
        return action


    def decay_epsilon(self):
        """
        Function that makes the epsilon decay. This decay can be linear if the initialization parameter
        linear_decay_mode is True(default) or exponential otherwise. This function doesn't receives any parameter,
        it only uses the instance parameters (self.):
            e_min :float
                Minimum value of epsilon
            e_lin_decay:
                Number of frames that the function will reach its minimum
            steps_cont:
                Current number of frames
            i_episode:
                Current number of episodes
                
        :param
            nothing
        :return
            nothing
        """
        if self.linear_decay_mode:
            # straight line equation wrapper by max operation -> max(min_value,(-mx + b))
            self.epsilon = np.amax((self.e_min, -((1.0-self.e_min)* self.steps_cont)/self.e_lin_decay + 1.0))
        else:
            # exponential's function C(e^-t) wrapped by min function
            self.epsilon = np.amin((1, (self.e_min + (1.0-self.e_min) * np.exp(-(self.i_episode-1) / self.e_exp_decay))))


    def refresh_history(self,history,state_next):
        """
        Function that updates the history (a set of "n" frames that is used as a state of the replay memory)
        taking out the first frame, moving the rest and adding the new frame to end of the history.

        :param:
            history : input volume of shape input_shape
                The history that will be stored (a set of n frames) as a state on the replay memory.
            state_next : Image
                Frame of the environment current state after a action was take.
        :return
            nothing
        """
        history[:,:,:-1] = history[:,:,1:]
        history[:,:,-1] = state_next[:,:,0]
        return history


    def train_dqn(self):
        """
        Function that trains the NN. It uniformly samples a sample from the replay memory. The replay memory returns
        a set composed of state,action,reward, state_next, done (if state_next is terminal), idx (iterator). These
        variables are used to feed the tensorflow graph that computes the loss and updates de weights.
        
        :param
            nothing
        :return
            nothing
        """
        # Uniform sampling from the replay memory
        st,act,r,st_next,d,idx = self.replay_memory.sample()
        self.loss += self.sess.run([self.train_op, self.loss_train],
                             feed_dict={self.state: st, self.action: act, self.reward: r,
                                        self.state_next: st_next, self.done: d})[1]


    def save_agent(self):
        """
        Function that saves the variables to plot in a .csv file and the weights in .h5 file.
        :param
            nothing
        :return
            nothing
        """
        folder_exists(self.path_save_weights)
        folder_exists(self.path_save_plot)
        self.Q_value.save_weights(os.path.join(self.path_save_weights,
                                               "weights-{}-{}.h5".format(self.env.getName(),self.steps_cont)))
        df = pd.DataFrame.from_dict(self.values_dict)
        self.reward_100=df["Rewards"].tail(100).mean()
        df.to_csv(os.path.join(self.path_save_plot,'-{}.csv'.format(self.env.getName())),index=False)


    def run(self, random_fill = False, to_render= False):
        """
        Function that runs the RL-DQN algorithm as demonstrated in the DQN paper.
        :param
            random_fill : bool (default false)
                Variable that decides if it is to fill the replay memory with states that come of random actions.
                If false the DQN algorithm will run otherwise it'll only choose random action e get the correspondent
                states from the environment to fill the replay memory.
            to_render : bool (default false)
                Variable that decides if it's to render on the screen the current episode.
                OBS: If this variable is true the fps will decrease harshly since it needs to
                show in reasonable speed the game.
        :return
            nothing
        """
        self.steps_cont=0
        time_it = time.time()
        self.i_episode = 0
        while self.steps_cont < self.num_simul_frames:
            self.i_episode += 1
            # Filling the replay memory until it reaches the number num_random_play
            if(random_fill and self.replay_memory.size()>=self.num_random_play):
                break
            state = self.env.reset()
            #Transforming the receive state (image frame) in a volume of n frames (history)
            state = np.concatenate((state,state,state,state),axis=2)
            #======Initializing variables====#
            done = False
            t = 0
            self.loss = 0
            self.q_rate = 0
            reward_total_episode = 0
            avg_loss = 0
            avg_q_rate = 0
            while not done:
                if (to_render and not random_fill):self.env.render()
                #the variable "t" differs from steps_cont on that it is reseted on each loop end
                t += 1
                #accomulate the total number of frames
                self.steps_cont += 1
                action = self.e_greddy_action(state.reshape((1,) + state.shape))
                state_next, reward, done, _ = self.env.step(action)
                state_next = self.refresh_history(np.copy(state), state_next)
                self.replay_memory.append(state,action,reward,state_next,done)
                state = np.copy(state_next)
                if not random_fill:
                    reward_total_episode += reward
                    self.train_dqn()
                    self.decay_epsilon()
                    if (self.steps_cont % self.target_update == 0):
                        printd("Q_hat was renewed!",lvl=2)
                        self.save_agent()
                        self.update_Q_hat()

            if not random_fill:
                avg_loss = self.loss / (t + 1)
                avg_q_rate = self.q_rate / (t + 1)
                self.values_dict["Rewards"].append(reward_total_episode)
                self.values_dict["Loss"].append(avg_loss)
                self.values_dict["Q_value"].append(avg_q_rate)
                self.values_dict["Num_frames"].append(self.steps_cont)
                self.values_dict["Time"].append(time.time()-time_it)

            fps= t/(time.time()-time_it)
            printd("Episode {:d}, Total Frames:{:d}/{:d}, Frames in this episode:{:d} , Total reward :{:.2f}, "
                   "Mean reward of {:d} frames:{:.3f}"
                   " Epsilon:{:.4f}, Replay memory size:{:d}, Loss:{:.4f}, Mean Q value:{:.4f}, FPS:{:.2f}, "
                   "Time of this episode:{:.3f}(s)".format(self.i_episode,
                                        self.steps_cont,self.num_simul_frames, t, reward_total_episode,
                                        self.target_update, self.reward_100,self.epsilon,
                                        self.replay_memory.size(),avg_loss,avg_q_rate,fps,time.time()-time_it),lvl=1)
            time_it = time.time()


if __name__ == "__main__":
    dqn = AgentDQN(env='PongNoFrameskip-v4', lr=1e-4,num_states_stored=500000)
    dqn.env.set_seed(seed)
    dqn.run(random_fill=True)
    dqn.run(to_render=False)